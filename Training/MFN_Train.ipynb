{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow_addons as tfa\n",
    "from solve_cudnn_error import *\n",
    "import math\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense \n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, PReLU, SeparableConv2D, DepthwiseConv2D, add, Flatten, Dropout\n",
    "\n",
    "solve_cudnn_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 囊括TFRecord進來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_train_dir = 'P:/3.軟體開發/1.演算法/Project/FaceAgeGenderRecognition/Outputs/TFRecords/train/'\n",
    "train_tfrecord = [tfrecord_train_dir + tfrec for tfrec in os.listdir(tfrecord_train_dir)]\n",
    "\n",
    "tfrecord_test_dir = 'P:/3.軟體開發/1.演算法/Project/FaceAgeGenderRecognition/Outputs/TFRecords/test/'\n",
    "test_tfrecord = [tfrecord_test_dir + tfrec for tfrec in os.listdir(tfrecord_test_dir)]\n",
    "# print(train_tfrecord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### If you wanna use pre-trained model\n",
    "model = tf.keras.models.load_model('.././Model/Backbone/MFN_62_075_gender_pre-trained.h5')\n",
    "prelu_output = model.get_layer('p_re_lu_34').output\n",
    "model_output = model.get_layer('dense_2').output\n",
    "\n",
    "M2 = Dropout(rate = 0.2, name = 'dropout_2')(prelu_output)\n",
    "M2 = Flatten(name = 'flatten_2')(M2)\n",
    "\n",
    "M2 = Dense(128, activation = None, use_bias = False, kernel_initializer = 'glorot_normal', name = 'dense_3')(M2)\n",
    "\n",
    "Y2 = Dense(units = 48, activation = 'softmax', name='age_output')(M2)\n",
    "\n",
    "model = Model(inputs = model.input, outputs = [Y2 ,model_output], name = 'customed_model')\n",
    "\n",
    "#### If you wanna train your model from scratch\n",
    "# sys.path.append('.././Model/Backbone/')\n",
    "# from MobileFaceNet import *\n",
    "# model = mobile_face_net_train(2, 0.75, 'softmax')\n",
    "# prelu_output = model.get_layer('p_re_lu_33').output\n",
    "# model_output = model.get_layer('dense_1').output\n",
    "\n",
    "# M2 = Dropout(rate = 0.2, name = 'dropout_2')(prelu_output)\n",
    "# M2 = Flatten(name = 'flatten_2')(M2)\n",
    "\n",
    "# M2 = Dense(128, activation = None, use_bias = False, kernel_initializer = 'glorot_normal', name = 'dense_3')(M2)\n",
    "\n",
    "# Y2 = Dense(units = 48, activation = 'softmax', name='age_output')(M2)\n",
    "\n",
    "# model = Model(inputs = model.input, outputs = [Y2 ,model_output], name = 'customed_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 對TFRecord進行解碼與預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_example_train(example_string): # 將 TFRecord 文件中的每一個序列化的 tf.train.Example 解碼，並且進行預處理\n",
    "    feature_dict = tf.io.parse_single_example(example_string, feature_description)\n",
    "    \n",
    "    feature_dict['image'] = tf.io.decode_jpeg(feature_dict['image'])    # 解碼JPEG圖片  \n",
    "    # Datatype is int8\n",
    "    x = tf.random.uniform(shape=[], minval=0.1, maxval=45.0)*0.0174532925\n",
    "    feature_dict['image'] = tfa.image.rotate(feature_dict['image'], x)\n",
    "    feature_dict['image'] = tf.image.random_brightness(feature_dict['image'], 0.2)\n",
    "    feature_dict['image'] = tf.image.random_hue(feature_dict['image'], max_delta=0.03)\n",
    "    feature_dict['image'] = tf.image.random_contrast(feature_dict['image'], 0.5, 1.5)\n",
    "    feature_dict['image'] = tf.image.random_saturation(feature_dict['image'], 0.5, 1.3)\n",
    "    feature_dict['image'] = tf.image.random_flip_left_right(feature_dict['image'])\n",
    "    \n",
    "    \n",
    "    # Datatype is Float32\n",
    "    feature_dict['image'] = tf.image.convert_image_dtype(feature_dict['image'], tf.float32)\n",
    "    feature_dict['image'] = tf.image.resize(feature_dict['image'], [62, 62])  # Resize 圖片\n",
    "    \n",
    "    feature_dict['Age'] = feature_dict['Age'] - 18  # 將 Age 扣掉 18，達成 48 分類\n",
    "#     feature_dict['Age'] = tf.one_hot(feature_dict['Age'], 48)\n",
    "    \n",
    "    feature_dict['Gender'] = tf.one_hot(feature_dict['Gender'], 2)\n",
    "    \n",
    "    return feature_dict['image'], feature_dict['Age'], feature_dict['Gender']\n",
    "\n",
    "def _parse_example_test(example_string): # 將 TFRecord 文件中的每一個序列化的 tf.train.Example 解碼，並且進行預處理\n",
    "    feature_dict = tf.io.parse_single_example(example_string, feature_description)\n",
    "    \n",
    "    feature_dict['image'] = tf.io.decode_jpeg(feature_dict['image'])    # 解碼JPEG圖片  \n",
    "    \n",
    "    # Datatype is Float32\n",
    "    feature_dict['image'] = tf.image.convert_image_dtype(feature_dict['image'], tf.float32)\n",
    "    feature_dict['image'] = tf.image.resize(feature_dict['image'], [62, 62])  # Resize 圖片\n",
    "    \n",
    "    feature_dict['Age'] = feature_dict['Age'] - 18  # 將 Age 扣掉 18，達成 48 分類\n",
    "#     feature_dict['Age'] = tf.one_hot(feature_dict['Age'], 48)\n",
    "    \n",
    "    feature_dict['Gender'] = tf.one_hot(feature_dict['Gender'], 2)\n",
    "    \n",
    "    return feature_dict['image'], feature_dict['Age'], feature_dict['Gender']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 告知解碼器每個Feature的類型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = { # 定義Feature結構，告訴解碼器每個Feature的類型是什麼\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    'Age': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'Gender': tf.io.FixedLenFeature([], tf.int64)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自訂Batch組成、Training Loop以及一些訓練過程配置設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 讀取多個TfRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = []\n",
    "for i in range(0, 4):\n",
    "    raw_dataset.append(tf.data.TFRecordDataset([train_tfrecord[i]]).map(_parse_example_train).repeat().batch(8).prefetch(4))\n",
    "\n",
    "for i in range(4, 8):\n",
    "    raw_dataset.append(tf.data.TFRecordDataset([train_tfrecord[i]]).map(_parse_example_train).repeat().batch(6).prefetch(4))\n",
    "    \n",
    "for i in range(8, 16):\n",
    "    raw_dataset.append(tf.data.TFRecordDataset([train_tfrecord[i]]).map(_parse_example_train).repeat().batch(1).prefetch(2))\n",
    "  \n",
    "for i in range(16, 20):\n",
    "    raw_dataset.append(tf.data.TFRecordDataset([train_tfrecord[i]]).map(_parse_example_train).repeat().batch(5).prefetch(4))\n",
    "    \n",
    "for i in range(20, 24):\n",
    "    raw_dataset.append(tf.data.TFRecordDataset([train_tfrecord[i]]).map(_parse_example_train).repeat().batch(5).prefetch(4))\n",
    "    \n",
    "for i in range(24, 32):\n",
    "    raw_dataset.append(tf.data.TFRecordDataset([train_tfrecord[i]]).map(_parse_example_train).repeat().batch(3).prefetch(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 讀取多個Test TfRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test_dataset = []\n",
    "for i in range(0, 2):\n",
    "    raw_test_dataset.append(tf.data.TFRecordDataset([test_tfrecord[i]]).map(_parse_example_test).batch(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定義Loss Function、Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_obj_age = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "loss_obj_gender = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "\n",
    "loss_age = tf.keras.metrics.Mean(name='age loss')\n",
    "loss_gender = tf.keras.metrics.Mean(name='gender loss')\n",
    "\n",
    "error_age = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "error_gender = tf.keras.metrics.CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 將訓練過程繪製成圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_process(train_age_loss_results, train_age_accuracy_results, train_gender_loss_results, train_gender_accuracy_results):\n",
    "    fig, axes = plt.subplots(2, 2, sharex=False, figsize=(20, 10))\n",
    "    fig.suptitle('Training Metrics', fontsize=35)\n",
    "\n",
    "    axes[0][0].set_ylabel(\"Age Loss\", fontsize=14)\n",
    "    axes[0][0].set_xlabel(\"Step\", fontsize=14)\n",
    "    axes[0][0].plot(train_age_loss_results, color = 'r')\n",
    "\n",
    "    axes[1][0].set_ylabel(\"Age Accuracy\", fontsize=14)\n",
    "    axes[1][0].set_xlabel(\"Step\", fontsize=14)\n",
    "    axes[1][0].plot(train_age_accuracy_results)\n",
    "    \n",
    "    axes[0][1].set_ylabel(\"Gender Loss\", fontsize=14)\n",
    "    axes[0][1].set_xlabel(\"Step\", fontsize=14)\n",
    "    axes[0][1].plot(train_gender_loss_results, color = 'r')\n",
    "\n",
    "    axes[1][1].set_ylabel(\"Gender Accuracy\", fontsize=14)\n",
    "    axes[1][1].set_xlabel(\"Step\", fontsize=14)\n",
    "    axes[1][1].plot(train_gender_accuracy_results)\n",
    "    \n",
    "    fig.savefig('training_step.jpg')\n",
    "    fig.clf()\n",
    "    plt.close(fig)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, y_age, y_gender):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_age, pred_gender = model(inputs)\n",
    "        age_loss = loss_obj_age(y_age, pred_age)\n",
    "        gender_loss = loss_obj_gender(y_gender, pred_gender)\n",
    "\n",
    "    gradients = tape.gradient([age_loss, gender_loss], model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    loss_age(age_loss)\n",
    "    loss_gender(gender_loss)\n",
    "\n",
    "    error_age(y_age, pred_age)\n",
    "    error_gender(y_gender, pred_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------\n",
      " Start training...\n",
      " Notice: Every 10000 steps will save a training step plot, training model and test the testing set.\n",
      "--------------------------------------------------------------------\n",
      "\n",
      " Step  1, ------ Total Loss: 4.18108, ------ Age_Loss: 3.87442, ------ Gender_Loss: 0.30666, ------ Age_Acc: 0.06250, ------ Gender_Acc: 0.89062\n",
      " Saving model...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = '../Results/Training_Checkpoints/MFN_Recognition_0.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4feb69f6c0f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n Saving model...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../Results/Training_Checkpoints/MFN_Recognition_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mtrain_gender_accuracy_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_gender\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\realtime\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     \"\"\"\n\u001b[0;32m   1051\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[1;32m-> 1052\u001b[1;33m                     signatures, options)\n\u001b[0m\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\realtime\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m    133\u001b[0m           'or using `save_weights`.')\n\u001b[0;32m    134\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[1;32m--> 135\u001b[1;33m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[0;32m    136\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\realtime\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\realtime\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\realtime\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;31m# Open in append mode (read/write).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to create file (unable to open file: name = '../Results/Training_Checkpoints/MFN_Recognition_0.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302)"
     ]
    }
   ],
   "source": [
    "t = 1\n",
    "train_age_loss_results = []\n",
    "train_age_accuracy_results = []\n",
    "train_gender_loss_results = []\n",
    "train_gender_accuracy_results = []\n",
    "\n",
    "print('\\n--------------------------------------------------------------------')\n",
    "print(\" Start training...\")\n",
    "print(\" Notice: Every 10000 steps will save a training step plot, training model and test the testing set.\")\n",
    "print('--------------------------------------------------------------------\\n')\n",
    "\n",
    "for (image1, age1, gender1), (image2, age2, gender2), (image3, age3, gender3), (image4, age4, gender4), (image5, age5, gender5),(image6, age6, gender6),(image7, age7, gender7),(image8, age8, gender8),(image9, age9, gender9),(image10, age10, gender10),(image11, age11, gender11),(image12, age12, gender12),(image13, age13, gender13),(image14, age14, gender14),(image15, age15, gender15),(image16, age16, gender16), (image17, age17, gender17), (image18, age18, gender18), (image19, age19, gender19), (image20, age20, gender20), (image21, age21, gender21),(image22, age22, gender22),(image23, age23, gender23),(image24, age24, gender24),(image25, age25, gender25),(image26, age26, gender26),(image27, age27, gender27),(image28, age28, gender28),(image29, age29, gender29),(image30, age30, gender30),(image31, age31, gender31),(image32, age32, gender32) in zip(*raw_dataset):\n",
    "\n",
    "    combine_image = tf.concat([image1, image2, image3, image4, image5, image6, image7, image8, image9, image10, image11, image12, image13, image14, image15, image16, image17, image18, image19, image20, image21, image22, image23, image24, image25, image26, image27, image28, image29, image30, image31, image32], 0)\n",
    "    combine_age = tf.concat([age1, age2, age3, age4, age5, age6, age7, age8, age9, age10, age11, age12, age13, age14, age15, age16, age17, age18, age19, age20, age21, age22, age23, age24, age25, age26, age27, age28, age29, age30, age31, age32], 0)\n",
    "    combine_gender = tf.concat([gender1, gender2, gender3, gender4, gender5, gender6, gender7, gender8, gender9, gender10, gender11, gender12, gender13, gender14, gender15, gender16, gender17, gender18, gender19, gender20, gender21, gender22, gender23, gender24, gender25, gender26, gender27, gender28, gender29, gender30, gender31, gender32], 0)\n",
    "\n",
    "    indices = tf.range(start=0, limit=tf.shape(combine_age)[0], dtype=tf.int32)\n",
    "    idx = tf.random.shuffle(indices)\n",
    "\n",
    "    combine_image = tf.gather(combine_image, idx)\n",
    "    combine_age = tf.gather(combine_age, idx)\n",
    "    combine_gender = tf.gather(combine_gender, idx)\n",
    "\n",
    "    train_step(combine_image, combine_age, combine_gender)\n",
    "    \n",
    "    template = 'Step {:>2}, ------ Total Loss: {:>4.5f}, ------ Age_Loss: {:>4.5f}, ------ Gender_Loss: {:>4.5f}, ------ Age_Acc: {:>4.5f}, ------ Gender_Acc: {:>4.5f}'\n",
    "    print('\\r', template.format(t+1, loss_age.result() + loss_gender.result(), loss_age.result(), loss_gender.result(), error_age.result(), error_gender.result()), end = '')\n",
    "\n",
    "    if t % 10000 == 0:\n",
    "        print('\\n Saving model...')\n",
    "        model.save('./Results/Training_Checkpoints/MFN_Recognition_' + str(t) + '.h5')\n",
    "        \n",
    "        train_gender_accuracy_results.append(error_gender.result())\n",
    "        train_gender_loss_results.append(loss_gender.result())\n",
    "\n",
    "        train_age_accuracy_results.append(error_age.result())\n",
    "        train_age_loss_results.append(loss_age.result())\n",
    "        \n",
    "        plot_training_process(train_age_loss_results, train_age_accuracy_results, train_gender_loss_results, train_gender_accuracy_results)\n",
    "        \n",
    "        print('--------------------------------------------------------------------')\n",
    "        print(' Using current model to test the testing set...')\n",
    "        \n",
    "        itera = 0\n",
    "        number = 0\n",
    "        bar = 50\n",
    "        \n",
    "        for (image_Asian_test, age_Asian_test, gender_Asian_test), (image_UTK_test, age_UTK_test, gender_UTK_test) in zip(*raw_test_dataset):\n",
    "        \n",
    "            combine_image_test = tf.concat([image_Asian_test, image_UTK_test], 0)\n",
    "            combine_age_test = tf.concat([age_Asian_test, age_UTK_test], 0)\n",
    "            combine_gender_test = tf.concat([gender_Asian_test, gender_UTK_test], 0)\n",
    "            \n",
    "            pred_age_test, pred_gender_test = model(combine_image_test)\n",
    "            \n",
    "            age_test_loss = loss_obj_age(combine_age_test, pred_age_test)\n",
    "            gender_test_loss = loss_obj_gender(combine_gender_test, pred_gender_test)\n",
    "            \n",
    "            loss_age(age_test_loss)\n",
    "            loss_gender(gender_test_loss)\n",
    "            \n",
    "            error_age(combine_age_test, pred_age_test)\n",
    "            error_gender(combine_gender_test, pred_gender_test)\n",
    "            \n",
    "            print('\\r Processing[' + '■' *number + ' '*(bar-1-number) + ']', end='')\n",
    "            \n",
    "            itera+=1\n",
    "            \n",
    "            if itera == int(800/bar):\n",
    "                number+=1\n",
    "                itera = 0\n",
    "            \n",
    "        template_test = 'Test Age Loss: {:>4.5f}, ------ Test Gender Loss: {:>4.5f}, ------ Test Age Accuracy: {:>4.5f}, ------ Test Gender Accuracy: {:>4.5f}'\n",
    "        print('\\n',template_test.format(loss_age.result(), loss_gender.result(), error_age.result(), error_gender.result()))\n",
    "        print('--------------------------------------------------------------------')\n",
    "        \n",
    "    loss_age.reset_states()\n",
    "    loss_gender.reset_states()\n",
    "\n",
    "    error_age.reset_states()\n",
    "    error_gender.reset_states()\n",
    "    \n",
    "    t+=1\n",
    "    \n",
    "        \n",
    "#     for image, agee, genderr in zip(combine_image, combine_age, combine_gender):\n",
    "#         print(label)\n",
    "#         print(agee.numpy())\n",
    "#         print(genderr.numpy())\n",
    "#         print(\"第\", str(t), \"張圖片\")\n",
    "#         plt.imshow(image.numpy())\n",
    "#         plt.show()\n",
    "#         t+=1\n",
    "#     print('------------------------------------------------------------------------')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
